{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "import scipy\n",
    "import tensorflow as tf\n",
    "import multiprocessing\n",
    "import keras\n",
    "import argparse\n",
    "\n",
    "from keras.models    import Sequential\n",
    "from keras.models    import Model, load_model\n",
    "from keras.layers    import Dense, Dropout, Flatten, BatchNormalization, Input, Embedding, LSTM, GRU\n",
    "from keras.layers    import Conv2D, MaxPooling2D, AveragePooling2D, Bidirectional, Lambda\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard\n",
    "from keras           import backend as K\n",
    "from keras.utils     import to_categorical\n",
    "\n",
    "from sklearn.utils           import shuffle\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics         import log_loss\n",
    "from sklearn.preprocessing   import OneHotEncoder\n",
    "\n",
    "#from sklearn.preprocessing import normalize\n",
    "\n",
    "#tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sets = np.load('./test_sets.pkl')\n",
    "train_test_index = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test model on current test set\n",
    "test_set = test_sets[train_test_index]\n",
    "\n",
    "# load model to allow for input of np array from TFRecords\n",
    "\n",
    "# NOTE* this reloading of the model is redundant, but the author found \n",
    "# no way to consume TFRecords in the testing of keras model \n",
    "smodel = tf.keras.models.load_model('best_best_rescnnqstft3_b128.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thanks to https://github.com/Hvass-Labs/TensorFlow-Tutorials/blob/master/18_TFRecords_Dataset_API.ipynb\n",
    "\n",
    "# Data in the form of TFRecords needs to be parsed before it is consumed by model\n",
    "# In this case, the mp3 spectrograms and one hot encoded labels have been\n",
    "# serialized into binary strings\n",
    "\n",
    "# parse reads the serialized data\n",
    "\n",
    "def parse(serialized):\n",
    "    # Define a dict with the data-names and types we expect to find in the\n",
    "    # TFRecords file. It is a bit awkward that this needs to be specified again,\n",
    "    # because it could have been written in the header of the TFRecords file\n",
    "    # instead.\n",
    "    features =         {\n",
    "            'image': tf.FixedLenSequenceFeature([], tf.float32, allow_missing = True),\n",
    "            'label': tf.FixedLenSequenceFeature([], tf.float32, allow_missing = True)\n",
    "        }\n",
    "    # Parse the serialized data so we get a dict with our data.\n",
    "    parsed_example = tf.parse_single_example (\n",
    "                                                    serialized = serialized,\n",
    "                                                    features   = features\n",
    "                                                )\n",
    "    image = parsed_example['image']\n",
    "\n",
    "    # Get the label associated with the image.\n",
    "    label = parsed_example['label']\n",
    "\n",
    "    # The image and label are now correct TensorFlow types.\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# will create a tf.dataset for optimized consumption of training data\n",
    "# the returned image, label tuple represents a call to iterator.get_next()\n",
    "\n",
    "def create_dataset(filepath):\n",
    "    \n",
    "    # This works with arrays as well\n",
    "    dataset = tf.data.TFRecordDataset(filepath)\n",
    "    \n",
    "    # Maps the parser on every filepath in the array. You can set the number of parallel loaders here\n",
    "    dataset = dataset.map(parse, num_parallel_calls=8)\n",
    "    \n",
    "    # This dataset will go on forever\n",
    "    dataset = dataset.repeat(1)\n",
    "    \n",
    "    # Set the number of datapoints you want to load and shuffle \n",
    "    dataset = dataset.shuffle(100)\n",
    "    \n",
    "    # Set the batchsize\n",
    "    dataset = dataset.batch(1)\n",
    "    \n",
    "    # Create an iterator\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    \n",
    "    # Create your tf representation of the iterator\n",
    "    next_element = iterator.get_next()\n",
    "\n",
    "#     # Bring your picture back in shape\n",
    "#     image = tf.reshape(image, [-1, 128, 128, 1])\n",
    "    \n",
    "#     # Create a one hot array for your labels\n",
    "#     #label = tf.one_hot(label, 5)\n",
    "#     label = tf.reshape(label, [-1,5])\n",
    "\n",
    "    \n",
    "    return next_element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_sets[train_test_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n"
     ]
    }
   ],
   "source": [
    "# load TFRecord data from test set and convert to np.array to consumed by \n",
    "# model for testing\n",
    "acc = []\n",
    "next_element = create_dataset(test_sets[train_test_index])\n",
    "\n",
    "for i in range(37):\n",
    "    print(i)\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "    \n",
    "        test_element = sess.run(next_element)\n",
    "        image = np.asarray(test_element[0]).reshape(150,128,128,1)\n",
    "        label = np.asarray(test_element[1]).reshape(150,5)\n",
    "    res = smodel.test_on_batch(image, label)\n",
    "    acc.append(res[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./results/pool/pool_MaxPooling2Dfold_5_test_acc.pkl\n"
     ]
    }
   ],
   "source": [
    "expt_folder_name = 'pool/'\n",
    "results_name     = ('pool_' + \n",
    "                    str('MaxPooling2D').replace('.','') +\n",
    "                    'fold_' + \n",
    "                    str(train_test_index+1) + \n",
    "                    '_test_acc' + \n",
    "                    '.pkl')\n",
    "results_path     = './results/' + expt_folder_name + results_name\n",
    "\n",
    "print(results_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.68630624"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.asarray(acc).dump(results_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
